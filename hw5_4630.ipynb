{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw5_4630.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMtDFSHq54WgZJT80KbXiu5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fuanonemus/cop4630spring2020/blob/master/hw5_4630.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bizWFkRYnKre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf2maHNH16ZL",
        "colab_type": "text"
      },
      "source": [
        "<h1>Terminology</h1>\n",
        "\n",
        "Artificial Intelligence - when a computer can do things that usually require human intelligence, ex. vision, recognition, and decision making\n",
        "\n",
        "Machine learning is a subset of AI and deep-learning is a subset of machine learning.\n",
        "\n",
        "Machine Learning - when the program adjusts itself in response to the data it's exposed to without human intervention. You train a model to make useful predictions using a data set.\n",
        "\n",
        "Two types of problems:\n",
        "* Supervised learning\n",
        "  * algorithm is given labeled training data\n",
        "  * during training, the algorithm determines the relationship between the features and the labels\n",
        "* unsupervised learning\n",
        "  * algorithm is given data and has to come up with labels\n",
        "  * goal is to find meaningful patterns in the data\n",
        "\n",
        "Reinforcement learning - you give the algorithm a goal, like winning a game, and it takes actions until it succeeds\n",
        "\n",
        "Label - whaat we want to predict, like a type or a value\n",
        "\n",
        "Feature - input variable used in the prediction, like an image or a value\n",
        "\n",
        "Example - instance of data given to the model\n",
        "\n",
        "Training - creating or learning the relationship between features and labels, building of the model\n",
        "\n",
        "Inference/Testing - applying the trained model to unlabeled examples to make predictions\n",
        "\n",
        "Regression Model - predicts continuous values like prices or probabilities\n",
        "\n",
        "Classification model - predicts discrete values like types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymdegS7J5mcH",
        "colab_type": "text"
      },
      "source": [
        "<h1>Linear Regression</h1>\n",
        "For modeling continuous values linearly.\n",
        "\n",
        "Equation:       y' = b + $w_1$$x_1$\n",
        "* y is the predicted label or desired output\n",
        "* b is the bias\n",
        "* w is the weight of the feature, how much impact it has\n",
        "* x is the feature\n",
        "\n",
        "You can have multiple features with multiple $wx$ terms.\n",
        "\n",
        "Training a model means finding good values for all the weights and the bias.\n",
        "\n",
        "Empirical Risk Minimization - in supervised learning, you iterate over lots of examples to find a model that minimizes loss\n",
        "\n",
        "Loss - value indicating how bad the model's prediction was on a single example. If you graphed all the data and your model line, the loss is the average of the distance of the points to your line.\n",
        "\n",
        "Mean Square Error (MSE) - average squared loss per example over the whole data set\n",
        "\n",
        "Iterative learning - finding the model that minimizes loss by:\n",
        "1. pick some random values for w\n",
        "2. calculate loss\n",
        "3. recalculate the values\n",
        "4. repeat 2-3 until the model converges (loss stops changing)\n",
        "\n",
        "<h2>Gradient Descent</h2>\n",
        "If you graph all the loss values with the weight values, you'll get a polynomial. The minimum of the polynomial is the weight value where the model converges.\n",
        "\n",
        "The gradient of loss is the derivative of the loss-weight curve. It has a direction and a magnitude. \n",
        "\n",
        "If your model has multiple features, thus multiple weights, the gradient is a vector of the partial derivatives with respect to the weights.\n",
        "\n",
        "To calculate the next weight value, add some fraction of the gradient's magnitude to the old weight value.\n",
        "\n",
        "Learning Rate - the step size used to determine the new weight value\n",
        "\n",
        "<h3>Stochastic Gradient Descent (SGD)</h3>\n",
        "A batch is the total number of examples used to calculate the gradient in a single iteration. Redundancy grows with batch size, it can be helpful for smoothening, but is more likely to slow down the process.\n",
        "\n",
        "SGD takes a single random example from the data set for each iteration. This requires a large number of iterations, and can be very noisy.\n",
        "\n",
        "Mini-batch SGD is when you choose a relatively small batch size. More efficient than full-batch and less noisy than SGD.\n",
        "\n",
        "Problems that might happen with gradient descent:\n",
        "* your learning rate is too high and you step past the minimum\n",
        "* your learning rate is too low and it takes forever\n",
        "* your model has multiple minimums and you get stuck in a local minimum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOrLwPm1s9jN",
        "colab_type": "text"
      },
      "source": [
        "<h2>Example of Linear Regression</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUBUTprAm3iN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first we need a data set\n",
        "xs = 2 * np.random.rand(100, 1)\n",
        "ys = 4 + 3 * xs + np.random.rand(100, 1)\n",
        "\n",
        "# then we split into training and testing\n",
        "trainin = xs[:80]\n",
        "trainout = ys[:80]\n",
        "testin = xs[80:]\n",
        "testout = ys[80:]\n",
        "\n",
        "# next we set the learning rate and the number of epochs (iterations)\n",
        "lr = 0.01\n",
        "epochs = 10\n",
        "\n",
        "# then we pick a random weight and bias to start with\n",
        "w = np.random.randn(1)\n",
        "b = np.zeros(1)\n",
        "\n",
        "# next we do iterative gradient descent\n",
        "for epoch in range(epochs):\n",
        "  for i in range(80):\n",
        "    # calculate value\n",
        "    y = w * trainin[i] + b\n",
        "\n",
        "    # calculate loss\n",
        "    gradientW = (y - trainout[i]) * trainin[i]\n",
        "    gradientB = (y - trainout[i])\n",
        "\n",
        "    # set new values by multiplying by step\n",
        "    w -= lr * gradientW\n",
        "    b -= lr * gradientB\n",
        "\n",
        "# finally we evaluate our model with test\n",
        "loss = 0\n",
        "for i in range(20):\n",
        "  loss += 0.5 * (w * testin[i] + b - testout[i]) ** 2\n",
        "loss /= 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C9HOJOktDGv",
        "colab_type": "text"
      },
      "source": [
        "<h2>Example with Mini-Batch Stochastic Gradient Descent</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSJUS8J9tWzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use the same data set, divisions, lr, epochs as before\n",
        "# for weight and bias in this example, i'll use vectorized code\n",
        "# this means makes it easier to add features, b = w_0 and x_0 = 1\n",
        "w_vector = np.random.randn(2,1)\n",
        "\n",
        "# because we're vectorizing, we need to add a column to x for bias\n",
        "trainin_vector = np.column_stack([np.ones((m, 1)), trainin])\n",
        "testin_vector = np.column_stack([np.ones((m,1)), testin])\n",
        "\n",
        "batch_size = 4\n",
        "for epoch in range(epochs):\n",
        "  # we need to come up with a random set of 4 values from the test set\n",
        "  # so start by shuffling the data so every time we pull a sample it \n",
        "  # will be random\n",
        "  indices = np.random.permutation(80)\n",
        "  shuffledxs = trainin_vector[indices]\n",
        "  shuffledys = trainout[indices]\n",
        "\n",
        "  for i in range(0, 80, batch_size):\n",
        "    # get sample from shuffled training set\n",
        "    xi = shuffledxs[i:i+batch_size]\n",
        "    yi = shuffledys[i:i+batch_size]\n",
        "\n",
        "    # calculate gradient and calculate new weights\n",
        "    gradient = 1 / batch_size * xi.T.dot(xi.dot(w_vector) - yi)\n",
        "    w_vector = w_vector - lr * gradient\n",
        "\n",
        "# evaluate the model\n",
        "loss = 0\n",
        "for i in range(20):\n",
        "  loss += 0.5 * (testin_vector[i].dot(w_vector) - testout[i]) ** 2\n",
        "loss /= 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByyNOl495uf5",
        "colab_type": "text"
      },
      "source": [
        "<h1>Models</h1>\n",
        "Network of layers. A directed and acyclic graph. The typology of the model defines the hypothesis space where the variation of weights is constrained.\n",
        "\n",
        "Topologies include: linear stack (single input to single output), two-branch, multi-head, and inception block\n",
        "\n",
        "After you define the architecture you pick the loss function and the optimizer.\n",
        "\n",
        "Loss/objective function - the quantity minimized during training.\n",
        "\n",
        "Optimizer - how the network is updated based on the loss function, some variant of stochastic gradient descent.\n",
        "\n",
        "Problem Type | Last-layer Activation | Loss Function\n",
        "--- | --- | ---\n",
        "Binary classification | sigmoid | binary_crossentropy\n",
        "multiclass, single-label classification | softmax | categorical_crossentropy\n",
        "multiclass, multi-label classification | sigmoid | binary_crossentropy\n",
        "regression to arbitrary values | none | mse\n",
        "regression to values in [0, 1] | sigmoid | mse or binary_crossentropy\n",
        "\n",
        "<h2>Keras</h2>\n",
        "Deep-learning framework for python with methods for defining and training deep-learning models\n",
        "\n",
        "Process:\n",
        "1. Define your training data: input tensors and target tensors\n",
        "2. Define a network of layers that map your inputs to your targets\n",
        "3. Configure the learning process by choosing a loss function, an optimizer, and some metrics to monitor\n",
        "4. Iterate on the training data with the fit() method\n",
        "\n",
        "<h2>Layers</h2>\n",
        "The fundamental data structure of a neural network. A data processing module. Input and output are tensors. Some layers are stateless, but most have weights learned through stochastic gradient descent.\n",
        "\n",
        "Different layers work for different formats:\n",
        "* vector data of 2D tensors is processed with densely/fully connected layers\n",
        "* sequence data in 3D tensors is processed with recurrent layers, ex. Long-short term memory (LSTM) layers\n",
        "* image data in 4D tensors is processed with 2D convolutional layers\n",
        "\n",
        "Each layer has certain shape limits for input and output. In Keras, layers are dynamic so they'll shift to match the shape of the next layer\n",
        "\n",
        "<h2>Data Sets</h2>\n",
        "In order to evaluate your model, you need a test set.\n",
        "\n",
        "Typically you split your data set into a training set and a testing set. The test set should be large enough to produce statistically significant results and it should be representative of the data set as a whole.\n",
        "\n",
        "The goal is to do as well on the training set as you do on the testing set. Never train on the test data or you'll ruin your evaluation metrics. \n",
        "\n",
        "One method for dividing the data set is Simple Hold-Out Validation. It's the simplest evaluation protocol. If your data set is small, the test set might not be big enough to be statistically significant. To identify this problem, shuffle the data before splitting and repeat for several rounds. If you get different metrics, your test set is too small. To fix this problem use k-fold validation or iterated k-fold validation (for especially small data sets). Split the data into k paritions of equal size. For each partition, train the model on the remaining k-1 partitions and evaluate using the initial partition. Average your metrics.\n",
        "\n",
        "If you divide your data set into three parts instead of two (training, validation, and testing), you'll further prevent overfitting. Use the validation set as a gateway to the test set. Use it to fine tune the model and only use the test set if your model has passed the validation set. \n",
        "\n",
        "<h2>Overfitting</h2>\n",
        "Occurs when a model adapts to the peculiarities of the training data in an effort to produce a low loss, but when given the test set produces a high loss. Caused by making a model more complex than necessary. \n",
        "\n",
        "Your model should be good a generalizing. Make sure:\n",
        "* to draw examples independently and identically at random from the dataset so they don't influence each other\n",
        "* the distribution of the data set is consistent (stationarity)\n",
        "* to draw examples from partitions of the same distribution\n",
        "\n",
        "If you violate these principles, pay careful attention to your evaluation metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1obieV7wjT2",
        "colab_type": "text"
      },
      "source": [
        "<h2>Example of Linear Regression with Keras Model</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB1TQgdywog4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for this example, we'll use the same data as before\n",
        "# first build the model by specifying the architecture\n",
        "# we only need one layer for one feature\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(1, input_shape=(1,)))\n",
        "\n",
        "# next specify the loss function and the optimizer\n",
        "model.compile(optimizer='sgd',loss='mse',metrics=['mse'])\n",
        "\n",
        "# train the model\n",
        "model.fit(trainin, trainout, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(testin, testout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlno1G0L540U",
        "colab_type": "text"
      },
      "source": [
        "<h1>Convolutional Neural Networks</h1>\n",
        "Used for image classification to progressively extract higher and higher level representations of an image. Take the image's pixel data as input and learns to extract features and their significance. \n",
        "\n",
        "Input: 3D tensor (feature map) the size of the image x 3 for RGB\n",
        "\n",
        "Anatomy:\n",
        "\n",
        "\n",
        "*   Stack of modules which extract features, perform the following operations:\n",
        "  *  Convolution\n",
        "  *  ReLU\n",
        "  *  Pooling\n",
        "*   Fully Connected Classification Layers\n",
        "\n",
        "<h2>Convolution</h2>\n",
        "Operation performed on feature maps to produce a more meaningful image.\n",
        "\n",
        "You take a filter and slide it over the input over every possible orientation and convolve the filter with the portion of overlapping input.\n",
        "\n",
        "Convolving is like if you made the matrices vectors and performed the dot product. You can add padding to increase the output size.\n",
        "\n",
        "Through training, the model finds optimal filter values for meaningful features. Increasing the number of filters will increase the number of features, but it will also increase the training time and each added filter will produce less information.\n",
        "\n",
        "<h2>ReLU</h2>\n",
        "A transformation on the map to add nonlinearity by getting rid of negative values.\n",
        "\n",
        "ReLU(x) = max(0, x)\n",
        "\n",
        "<h2>Pooling</h2>\n",
        "Downsamples the map to save time. One method is Max Pooling: You lay a grid over the map and only keep the largest value in each square. Take input value stride which defines the size of the squares of the grid.\n",
        "\n",
        "<h2>Fully Connected Layers</h2>\n",
        "Used for classification. Determine labels based on the feature data. \n",
        "\n",
        "Last layer is typically softmax, which outputs a probability value from 0 to 1 for each label.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x0nA87CRiWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here's how to build a convnet from scratch:\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "# add convolutional layers with pooling\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "\n",
        "# add final fully connected layers for classification\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# alternatively you can use a pretrained convnet from keras\n",
        "# all you need to do is import the desired convnet\n",
        "from keras.applications import VGG16\n",
        "\n",
        "# then you call its constructor\n",
        "base = VGG16(weights='imagenet',include_top=False,input_shape=(150,150,3))\n",
        "\n",
        "# and add the base to a new model\n",
        "model = models.Sequential()\n",
        "model.add(base)\n",
        "# then add in a dense classifier\n",
        "\n",
        "# you can also train only certain parts of the model by freezing layers\n",
        "base.trainable = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shrx8WOnQJdR",
        "colab_type": "text"
      },
      "source": [
        "<h1>Fine Tuning Issues</h1>\n",
        "\n",
        "Regularization - a technique for improving generalization in a model to prevent or less the effects of overfitting by adding a term to reduce the weights of features\n",
        "\n",
        "You can add a regularizer to a layer in keras by adding the kernel_regularizer parameter to a layer.\n",
        "\n",
        "Dropout - another technique for preventing overfitting by randomly droping inputs\n",
        "\n",
        "You can add a dropout layer in keras\n",
        "\n",
        "Data Augmentation - a technique used for training a large model if you don't have a lot of data\n",
        "\n",
        "You can use the keras ImageDataGenerator library to do this\n",
        "\n",
        "To fine tune a model try retraining it's layers by freezing some and unfreezing others."
      ]
    }
  ]
}